\documentclass{idc_msc}

\title{Natural Language Processing \\\large Lecture 3}
\date{2019-03-26 \\ Last edited \currenttime\ \today}
\author{Lecture by Dr. Kfir Bar\\Typeset by Steven Karas}

\AtBeginDocument{\maketitle}
\AtEndDocument{\vfill\bibliographystyle{plain}\bibliography{../biblio}{}}

\usepackage{tikz-qtree}

\begin{document}

\paragraph{Disclaimer}

These notes are based on the lectures for the course Natural Language Processing, taught by Dr. Kfir Bar at IDC Herzliyah in the spring semester of 2018/2019.
Sections may be based on the lecture slides prepared by Dr. Kfir Bar.

\nocite{Jurafsky:2009:SLP:0131873210}
\nocite{manning1999foundations}
\nocite{DBLP:journals/corr/Goldberg15c}

\section{Homework}

The homework is due tomorrow.

\section{Agenda}

\begin{itemize}
  \item Smoothing
\end{itemize}

\section{Smoothing}

A naive model is overly specialized, and we want to generalize.
Smoothing is an easy way to do that, by simply adding 1 to all the counters.

\subsection{Laplace Smoothing}

For all possible n-grams, add the count of one:

\[
  \Pr[w_n \mid w_1^{n-1}] = \frac{C(w_n w_1^{n-1}) + 1}{C(w_1^{n-1}) + V}
\]

Where \(C\) is a count of n-grams, \(N\) is the count of history, and \(V\) is the vocabulary size.

In practice, \(V\) is often a hyperparameter as some function of the number of types.
Similarly, rather than adding \(1\), we add some smaller \(\lambda\) to avoid punishing very established probabilities.

\subsection{Witten-Bell smoothing}

An unseen n-gram is one that just hasn't been seen yet.
If we were given sufficient data, we would expect to see it.
We can estimate this based on the size of our corpus.

Let \(N\) be the number of tokens, \(T\) be the number of seen types, and \(V\) be the number of known types.
Define \(Z\) as the number of unseen types that are in the vocabulary:

\[
  Z = \sum_{i : c_i = 0} 1
\]

This gives the probability of seen unigrams:

\[
  p_i^* = \frac{c_i}{N+T}
\]

The probability of unseen unigrams would then be:

\[
  p_i^* = \frac{T}{Z(T + N)}
\]

Or the bigram case:

\[
\begin{aligned}
  p^*_{\text{unseen}}(w_i \mid w_x) &= \frac{T(w_x)}{Z(w_x)(N(w_x) + T(w_x))} \\
  p^*_{\text{seen}}(w_i \mid w_x) &= \frac{c(w_x, w_i)}{N(w_x) + T(w_x)}
\end{aligned}
\]

\[
  Z(w_x) = V - T(w_x)
\]

\section{Backoff}

\subsection{Linear interpolation}

Solve the sparseness in large n-gram models by mixing them with weighted smaller n-gram models:

\[
  \Pr[w_n \mid w_1 \ldots w_{n-1}] = \sum_{i=0}^n \lambda_i \Pr[w_n \mid w_{n-i}\ldots w_{n-1}]
\]

\section{Part of speech tagging}

We want to tag the POS tags of every word in a given sentence.
We use tag sets to classify specific words according to their role in a sentence.
For example, part of the \href{http://www.ling.upenn.edu/histcorpora}{Penn Historical Treebank} tag set:

\begin{center}
\begin{tabu} to \linewidth{|r|l|r|l|}
\hline
Tag & Meaning & Tag & Meaning \\
\hline
\hline
NN & Noun & VBS & Verb - present \\
\hline
NNS & Noun - plural & VBD & Verb - past \\
\hline
NNP & Noun - proper & VBP & Verb - progressive \\
\hline
ADV / RB & Adverb & IN & preposition \\
\hline
DET & Definite article & ART & Indefinite article \\
\hline
ADJ & Adjective & ... & ... \\
\hline
\end{tabu}
\end{center}

Ambiguity is the primary obstacle to doing this perfectly.
For example, "Around" can be either a preposition, a particle, or an adverb.
As such, while humans can perform this task with near 100\% accuracy, while SOTA achieves close to 97\% accuracy.

In the worst case, several taggings are syntactically valid, yet semantically nonsense:

\begin{itemize}
  \item Time (VB) flies (NN) like (RB) an (ART) arrow (NN)
  \item Time (NN) flies (VB) like (RB) an (ART) arrow (NN)
  \item Time (ADJ) flies (NN) like (VB) an (ART) arrow (NN)
  \item Enraged (ADJ) cow (NN) injures (VB) farmer (NN) with (...) ax (NN) - a similar, yet different issue
\end{itemize}

\subsection{Supervised learning}

Given a tagged dataset, learn how to model and predict tags based on the given data.
There are many publicly available tagged datasets.

\subsubsection{Markov Models}

A Markov model is a state machine that assigns a probability to transition to each future state.
Such a model satisfies the Markovian property:

\[
  \Pr[s_{ik} \mid s_{i1}, \ldots, s_{ik-1}] = \Pr[s_{ik} \mid s_{ik-1}]
\]

Meaning that the current state is only dependent upon the immediately previous one.
We can define the adjacency matrix \(A\) of the graph as:

\[
  A_{ij} = \Pr[s_j \mid s_i]
\]

It is common and useful to augment this adjacency matrix with start and end sequence states.

The probability that we observe a sequence of states \((s_1, \ldots, s_t)\):

\[
  \Pr[s_1, \ldots, s_T] = \prod_{t=1}^T A_{s_{t-1}s_t}
\]

We will cover the use of MLEs on Markov models later on today.

\subsubsection{Hidden Markov Models}

Model observations into the model, we get that given a sequence of observations \(o\) and model how they map to a sequence of states \(s\):

\[
\begin{aligned}
  o &= (o_1, \ldots, o_T) \\
  s &= (s_1, \ldots, s_T)
\end{aligned}
\]

where the probability of seeing any particular state given an observation:

\[
  \Pr[s \mid o] =
  \frac{\Pr[o \mid s] \Pr[s]}{\Pr[o]} =
  \frac{\Pr[o \mid s] \Pr[s]}{\sum_{s'} \Pr[s', o]}
\]

Note that the denominator is constant with regards to \(s\), so we can ignore it.
To maximize the probability for sequences observed in a corpus:

\[
  \max_{A,B} \frac{\Pr[o \mid s; A, B] \Pr[s; A, B]}{\sum_{s'} \Pr[s', o; A, B]}
\]

In order to find the most likely state \(\hat{s}\) given an observation \(o\):

\[
\begin{aligned}
  \hat{s} &= \arg \max_s \Pr[s \mid o] \\
  &= \arg \max_s \Pr[o \mid s] \Pr[s] \\
  &= \arg \max_s \prod_{t = 1}^T B_{s_t o_t} \cdot A_{s_{t-1} s_t}
\end{aligned}
\]

\paragraph{Formal model}

\[
  M = (A, B, \pi)
\]

Where \(A\) is the state transition adjacency matrix, \(B\) is the emission matrix, and \(\pi\) is a vector of initial probabilities.\footnote{It is possible we accidentally swapped the order of the matrix multiplication}

It's important to remember that HMM does not output pure probabilities, but rather scores that cannot be compared.

\subsection{Viterbi algorithm}

A dynamic programming algorithm that runs in polytime.
Decoding is the process of finding the best state-sequence that generates the observations.

\[
\begin{aligned}
  \delta_j(t) &= \max_{s_1, \ldots, s_{t-1}} \Pr[s_1\ldots s_{t-1}, o_1 \ldots o_{t-1}, s_t = j, o_t] \\
  \delta_j(t+1) &= \max_i \delta_i(t) a_{ij}b_{jo_{i+1}} \\
  \delta_j(1) &= \pi_j b_{jo_1} \\
  \psi_j(t+1) &= \arg\max_i \delta_i(t) a_{ij} b_{j o_{t+1}}
\end{aligned}
\]

\subsection{Unsupervised learning}

Given a dataset without tags, learn features that model patterns.

\section{Next Week}

\end{document}
